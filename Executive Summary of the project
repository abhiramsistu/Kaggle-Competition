Insightful Data Exploration
We explored customer behaviors within the dataset.
Identified key variables like visit frequency and purchase 
history.
Noted the significance of days inactive and average visit 
duration.
These variables are pivotal in distinguishing churn patterns.
Insights guide predictive modeling for churn prediction

Data Preparation
Rigorous Data Preparation and Feature 
Engineering
❑ Feature engineering: Create a new feature 'interaction_clicks_visits' by 
dividing clicks by visits to capture user engagement.
❑ Convert data to matrix format: Prepare the data for XGBoost modeling by 
converting it into matrix format. Perform hyperparameter tuning: Use 
cross-validation to tune hyperparameters and find the best iteration using 
early stopping.
❑ Define parameters such as the objective function, maximum depth, 
learning rate (eta), regularization terms (lambda and alpha), etc.
❑ Train multiple XGBoost models: Use the bagging technique to train multiple 
XGBoost models on different subsets of the data with replacement
❑ Make predictions and ensemble: Generate predictions using each model 
and then ensemble them by averaging to improve predictive performance

Enhancing Churn Prediction with XGBoost:
Chosen for its superior performance in binary classification tasks, offering both speed and accuracy 
in processing complex datasets.
Conducted systematic tuning of parameters, including a conservative tree depth and learning rate, to 
ensure the model captures essential data patterns without overfitting.
Introduced calculated features like 'interaction_clicks_visits' to quantify customer engagement 
levels, significantly enhancing model input quality.
Adopted an iterative modeling process, allowing continuous refinement based on feature 
importance analysis to drop redundant variables and focus on those influencing churn prediction.

Evaluation
Rigorous Evaluation Methodology for Robust Insights
Implemented 5-fold stratified cross-validation to 
maintain the original target class distribution, ensuring 
robust model assessment.
AUC Performance: With an AUC of 0.74, the model 
effectively balances true positive and false positive rates, 
demonstrating robust predictive accuracy in classifying 
churn instances.
Compared XGBoost model against standard benchmarks 
to establish its superior predictive power in identifying Managerial Implications and 
Limitations
• Targeted interventions for high-risk churn customers, 
emphasizing engagement and personalized 
experiences.
• Highlighted the importance of visit frequency and 
inactivity as predictive factors, suggesting strategies 
for regular customer engagement.
• Acknowledged model limitations: need for continual 
updates and potential biases in historical data.
• Future directions: incorporate real-time data for 
dynamic prediction and explore ensemble learning for 
improved accuracy
potential churn.
Analysis of validation curves confirmed the model's 
consistent performance through training and its strong 
generalization to unseen data, underscoring its reliability 
for practical application.

